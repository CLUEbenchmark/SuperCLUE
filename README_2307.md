# SuperCLUE
中文通用大模型综合性基准SuperCLUE

SuperCLUE: A Benchmark for Foundation Models in Chinese


- [SuperCLUE是什么](#SuperCLUE是什么)
- [SuperCLUE的构成与特点](#SuperCLUE的构成与特点)
- [SuperCLUE的测试结果](#SuperCLUE的测试结果)
     - [总榜单](#2023年7月Superclue中文大模型总排行榜)
     - [基础能力表](#2023年7月SuperCLUE基础能力榜单)
     - [中文特性能力表](#2023年7月SuperCLUE中文特性榜单)
- [SuperCLUE的特点](#SuperCLUE的特点)
- [SuperCLUE的数据集](#SuperCLUE的数据集)
- [人类基准测评](#人类基准测评)
- [实验分析](#实验分析)
     - [人类与模型的对比](#人类与模型的对比)
     - [模型层面-宏观分析](#模型层面-宏观分析)
     - [能力角度分析](#能力角度分析) 
     - [国内大模型点评](#国内大模型点评) 
- [SuperCLUE的不足与局限](#SuperCLUE的不足与局限)
- [SuperCLUE的常见问答](#SuperCLUE的常见问答)
- [SuperCLUE讨论与交流](#SuperCLUE讨论与交流)

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/superclue.jpeg"  width="90%" height="90%"></img>

SuperCLUE基准计划按照月度进行更新，纳入更多可用中文大模型，欢迎联系与交流；数据集和进一步信息计划在下一次更新时公开，敬请期待。

News   
[23/07/25] 2023年7月SuperCLUE中文大模型排行榜发布

[23/07/05] 中文大模型的开放式问题与多轮对话基准发布，<a href='https://github.com/CLUEbenchmark/SuperCLUE-Open'>SuperCLUE-Open</a>

[23/05/31] <a href='https://www.SuperCLUEAI.com'>中文通用大模型匿名对战评价基准，SuperCLUE琅琊榜</a>
     
##### 更新
    更新 Update（2023-06-19），SuperCLUE中文大模型排行榜(2023年6月) 
    更新 Update（2023-05-30），添加RWKV-7B,IDEA-姜子牙-13B,西湖大模型v2       
    更新 Update（2023-05-21），添加360智脑
    更新 Update（2023-05-12）
    添加Claude: OpenAI最强竞争对手Anthropic的Claude取得了与ChatGPT3.5一致的效果
    添加ChatGLM-130B: 相比ChatGLM-6B效果大幅提升（+7.35），处于国内较好水平

## SuperCLUE是什么
中文通用大模型基准（SuperCLUE），是针对中文可用的通用大模型的一个测评基准。

它主要回答的问题是：在当前通用大模型大力发展的背景下，中文大模型的效果情况，包括但不限于"这些模型不同任务的效果情况"、"相较于国际上的代表性模型做到了什么程度"、
"这些模型与人类的效果对比如何"。

它尝试在一系列国内外代表性的模型上使用多个维度能力进行测试。SuperCLUE是中文语言理解测评基准（CLUE）在通用人工智能时代的进一步发展。

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/superclue_rank7.jpeg"  width="100%" height="100%"></img>


## SuperCLUE的测试结果
四个表格：汇总表、基础能力表、专业能力表、中文特性能力表

#####  排行榜会定期更新           数据来源: www.CLUEbenchmarks.com              

### 2023年7月SuperCLUE中文大模型总排行榜

| 排名 |                                                                    模型                                                                     |     机构     |  总分   | 基础能力 | 中文特性 | 学术专业 | 许可证 |
|:--:|:-----------------------------------------------------------------------------------------------------------------------------------------:|:----------:|:-----:|:---:|:---:|:---:|:---:|
| 🧝 |                                                                    人类                                                                     |    CLUE    | 83.66 | 85.03 | 82.29 | - | - |
| -  |                                                  <a href='https://openai.com/'>GPT-4</a>                                                  |   OpenAI   | 70.89 | 70.04 | 72.67 | 69.96 | 专有服务 |
| 🏅 |                                        <a href='https://yiyan.baidu.com/welcome'>文心一言(v2.2.0) </a>                                        |     百度     | 62.00 | 61.11 | 71.38 | 53.50 | 专有服务 |
| -  |                                            <a href='https://www.anthropic.com/'>Claude-2 </a>                                             | Authropic  | 60.94 | 62.01 | 61.18 | 59.63 | 专有服务 |
| -  |                                              <a href='https://openai.com/'>gpt-3.5-turbo</a>                                              |   OpenAI   | 59.79 | 64.40 | 63.19 | 51.78 | 专有服务 |
| 🥈 |                                               <a href='https://chatglm.cn'>ChatGLM-130B</a>                                               | 清华大学&智谱AI  | 59.35 | 53.78 | 71.39 | 52.89 | 专有服务 |
| 🥉 |                                            <a href='https://xinghuo.xfyun.cn/'>讯飞星火(v1.5)</a>                                             |    科大讯飞    | 58.02 | 63.32 | 65.72 | 45.03 | 专有服务 |
| -  |                                        <a href='https://www.anthropic.com/'>Claude-instant-v1</a>                                         | Authropic  | 56.31 | 58.85 | 55.91 | 54.16 | 专有服务 |
| 4  |                                                <a href='https://ai.360.cn'>360智脑(4.0)</a>                                                 |    360     | 55.04 | 56.68 | 62.54 | 45.88 | 专有服务 |
| 5  |                              <a href='https://huggingface.co/internlm/internlm-chat-7b'>internlm-chat-7b</a>                              | 上海AI实验室与商汤 | 53.91 | 54.85 | 61.35 | 45.53 | 开源-可商用 |
| 6  |                                      <a href='https://github.com/THUDM/ChatGLM2-6B'>ChatGLM2-6B</a>                                       | 清华大学&智谱AI  | 53.85 | 55.60 | 63.59 | 42.37 | 开源-可商用 |
| 7  |                                          <a href='https://api.minimax.chat/'>MiniMax-abab5.5</a>                                          |  MiniMax   | 53.06 | 53.61 | 62.79 | 42.77 | 专有服务 |
| 8  |                                           <a href='https://tongyi.aliyun.com/'>通义千问(v1.0.3)</a>                                           |    阿里巴巴    | 51.52 | 52.84 | 61.73 | 39.98 | 专有服务 |
| 9  |                           <a href='https://huggingface.co/baichuan-inc/Baichuan-13B-Chat'>Baichuan-13B-Chat</a>                           |    百川智能    | 49.35 | 50.46 | 55.38 | 42.21 | 开源-可商用 |
| 10 |                       <a href='https://huggingface.co/BelleGroup/BELLE-LLaMA-13B-2M-enc'>BELLE-LLaMA-13B-2M-enc</a>                       |     链家     | 46.60 | 48.71 | 52.99 | 38.10 | 开源-非商用 |
| 11 |                           <a href='https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1.1'>IDEA-姜子牙-13B-v1.1</a>                            | 深圳IDEA研究院  | 43.80 | 47.55 | 48.61 | 35.26 | 开源-非商用 |
| 12 |                         <a href='https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b'>phoenix-7B</a>                          |   香港中文大学   | 41.57 | 45.39 | 44.62 | 34.70 | 开源-可商用 |
| 13 |                  <a href='https://huggingface.co/fnlp/moss-moon-003-sft'>MOSS-16B</a>                                                     |    复旦大学        | 35.36      |  37.01     |  38.01     |  31.07     | 开源-可商用 |
| 14 |                              <a href='https://huggingface.co/meta-llama/Llama-2-13b-hf'>Llama-2-13B-chat</a>                              |    Meta    | 34.26 | 35.85 | 37.37 | 29.57 | 开源-可商用 |
| 15 |                                        <a href='https://github.com/lm-sys/FastChat'>Vicuna-13B</a>                                        |   UC伯克利    | 31.70 | 34.61 | 33.71 | 26.80 | 开源-非商用 |
| 16 |                             <a href='https://huggingface.co/BlinkDL/rwkv-4-world'>RWKV-7B-World-CHNtuned</a>                              |  RWKV基金会   | 27.83 | 30.71 | 28.13 | 24.66 | 开源-可商用|
   
    注：国外代表性非开源模型（GPT4.0/Claude/gpt-3.5）参与榜单，但不参与排名

   
 ### 2023年7月SuperCLUE开源榜单
  | 排名  |                                              模型                                               |     机构     |  总分   | 基础能力  | 中文特性  | 学术专业  | 许可证 |
|:---:|:---------------------------------------------------------------------------------------------:|:----------:|:-----:|:-----:|:-----:|:-----:|:---:|
| 🧝  |                                              人类                                               |    CLUE    | 83.66 | 85.03 | 82.29 |   -   | - |
| 🏅️ |        <a href='https://huggingface.co/internlm/internlm-chat-7b'>internlm-chat-7b</a>        | 上海AI实验室与商汤 | 53.91 | 54.85 | 61.35 | 45.53 | 开源-可商用 |
| 🥈  |                <a href='https://github.com/THUDM/ChatGLM2-6B'>ChatGLM2-6B</a>                 | 清华大学&智谱AI  | 53.85 | 55.60 | 63.59 | 42.37 | 开源-可商用 |
| 🥉  |     <a href='https://huggingface.co/baichuan-inc/Baichuan-13B-Chat'>Baichuan-13B-Chat</a>     |    百川智能    | 49.35 | 50.46 | 55.38 | 42.21 | 开源-可商用 |
|  4  | <a href='https://huggingface.co/BelleGroup/BELLE-LLaMA-13B-2M-enc'>BELLE-LLaMA-13B-2M-enc</a> |     链家     | 46.60 | 48.71 | 52.99 | 38.10 | 开源-非商用 |
|  5  |     <a href='https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1.1'>IDEA-姜子牙-13B-v1.1</a>      | 深圳IDEA研究院  | 43.80 | 47.55 | 48.61 | 35.26 | 开源-非商用 |
|  6  |   <a href='https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b'>phoenix-7B</a>    |   香港中文大学   | 41.57 | 45.39 | 44.62 | 34.70 | 开源-可商用 |
|  7  |                                    <a href='https://huggingface.co/fnlp/moss-moon-003-sft'>MOSS-16B</a>                                    |    复旦大学        | 35.36      |  37.01     |  38.01     |  31.07     | 开源-可商用 |
|  8  |        <a href='https://huggingface.co/meta-llama/Llama-2-13b-hf'>Llama-2-13B-chat</a>        |    Meta    | 34.26 | 35.85 | 37.37 | 29.57 | 开源-可商用 |
|  9  |                  <a href='https://github.com/lm-sys/FastChat'>Vicuna-13B</a>                  |   UC伯克利    | 31.70 | 34.61 | 33.71 | 26.80 | 开源-非商用 |
| 10  |       <a href='https://huggingface.co/BlinkDL/rwkv-4-world'>RWKV-7B-World-CHNtuned</a>        |  RWKV基金会   | 27.83 | 30.71 | 28.13 | 24.66 | 开源-可商用|
  
 往期榜单，
2023年6月：<a href='./README_2306.md'>2023年6月SuperCLUE中文特性榜单</a>

2023年5月：<a href='./README_2305.md'>2023年5月SuperCLUE中文特性榜单</a>

### 2023年7月SuperCLUE基础能力榜单

| 排名  |           模型           | 平均分 | 语义理解 | 闲聊 | 对话 | 角色扮演 | 知识与百科 | 生成与创作 | 逻辑与推理 | 代码 | 计算 | 安全 |
|:---:|:----------------------:| :---: | :---: | :---: | :---: | :---: | :---: | :---: |:-----:| :---: | :---: | :---: |
| 🧝  |           人类           | 85.03 | 90.17 | 71.53 | 77.99 | 82.19 | 97.44 | 68.79 | 90.55 | 90.45 | 94.97 | 86.22 |
|  -  |         gpt-4          | 70.04 | 82.91 | 46.77 | 66.39 | 63.46 | 92.65 | 66.67 | 60.33 | 85.45 | 61.48 | 73.02 |
|  -  |     gpt-3.5-turbo      | 64.40 | 87.18 | 45.16 | 65.57 | 60.58 | 85.29 | 72.36 | 42.98 | 72.73 | 38.52 | 72.22 |
| 🏅️ |       讯飞星火(v1.5)       | 63.32 | 78.26 | 45.90 | 59.84 | 55.88 | 73.48 | 54.92 | 54.70 | 60.00 | 76.86 | 71.54 |
|  -  |        Claude-2        | 62.01 | 83.49 | 49.59 | 57.14 | 52.88 | 78.68 | 68.07 | 53.72 | 66.06 | 44.26 | 65.60 |
| 🥈  |      文心一言(v2.2.0)      | 61.11 | 81.90 | 46.34 | 56.67 | 59.80 | 86.76 | 47.73 | 36.52 | 65.79 | 52.63 | 70.63 |
|  -  |   Claude-instant-v1    | 58.85 | 76.52 | 50.00 | 58.20 | 55.77 | 77.04 | 61.48 | 40.00 | 66.97 | 33.61 | 67.77 |
| 🥉  |       360智脑(4.0)       | 56.68 | 76.92 | 52.46 | 58.33 | 54.08 | 76.80 | 61.54 | 37.29 | 53.64 | 29.57 | 67.92 |
|  4  |      ChatGLM2-6B       | 55.60 | 74.36 | 44.35 | 55.74 | 56.73 | 76.47 | 51.22 | 40.50 | 41.82 | 45.08 | 66.67 |
|  5  |    internlm-chat-7b    | 54.85 | 80.34 | 48.39 | 55.74 | 55.77 | 77.94 | 36.59 | 37.19 | 51.82 | 34.43 | 68.25 |
|  6  |      ChatGLM-130B      | 53.78 | 70.94 | 45.97 | 56.56 | 61.54 | 75.74 | 55.28 | 29.75 | 45.45 | 31.15 | 63.49 |
|  7  |    MiniMax-abab5.5     | 53.61 | 79.49 | 45.97 | 59.84 | 60.58 | 85.29 | 47.97 | 29.75 | 30.00 | 31.97 | 61.11 |
|  8  |          通义千问          | 52.84 | 74.77 | 45.97 | 57.98 | 53.00 | 76.69 | 38.89 | 33.06 | 46.67 | 39.67 | 60.40 |
|  9  |   Baichuan-13B-Chat    | 50.46 | 64.10 | 41.94 | 50.00 | 52.88 | 75.00 | 57.72 | 27.27 | 40.91 | 31.15 | 60.32 |
| 10  |       BELLE-13B        | 48.71 | 68.38 | 46.77 | 51.64 | 53.85 | 64.71 | 25.20 | 32.23 | 48.18 | 31.97 | 63.49 |
| 11  |   IDEA-姜子牙-13B-v1.1    | 47.55 | 70.09 | 49.19 | 48.36 | 48.08 | 58.82 | 32.52 | 34.71 | 21.82 | 45.08 | 63.49 |
| 12  |       Phoenix-7B       | 45.39 | 66.67 | 41.94 | 43.44 | 43.27 | 55.15 | 44.72 | 31.41 | 36.36 | 33.61 | 55.56 |
| 13  |        MOSS-16B        | 37.01 | 54.70 | 39.52 | 40.16 | 45.19 | 35.29 | 34.96 | 24.79 | 32.73 | 27.05 | 37.30 |
| 14  |    Llama-2-13B-chat    | 35.85 | 52.14 | 41.94 | 40.98 | 32.69 | 33.82 | 38.21 | 28.93 | 23.64 | 27.05 | 38.10 |
| 15  |       Vicuna-13B       | 34.61 | 49.57 | 33.06 | 32.79 | 37.50 | 25.74 | 30.89 | 27.27 | 40.91 | 35.25 | 35.71 |
| 16  | RWKV-7B-World-CHNtuned | 30.71 | 31.62 | 20.16 | 22.13 | 26.92 | 27.21 | 23.58 | 22.31 | 36.36 | 60.66 | 36.51 |

    注：国外代表性非开源模型（GPT4.0/Claude/gpt-3.5）参与榜单，但不参与排名；指标为：Accuracy。数据表比较大，表格请往后拉        

### 2023年7月SuperCLUE中文特性榜单

| 排名  |           模型           | 平均分 | 字形和拼音 | 字义理解 | 句法分析 | 文学 | 诗词 | 成语 | 歇后语和谚语 | 方言 | 对联 | 古文 |
|:---:|:----------------------:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 🧝  |           人类           | 82.29 | 96.01 | 83.15 | 62.71 | 91.47 | 90.79 | 92.38 | 83.78 | 69.21 | 70.00 | 83.40 |
|  -  |         gpt-4          | 72.67 | 62.83 | 68.07 | 85.48 | 88.08 | 75.68 | 95.12 | 70.15 | 38.40 | 71.52 | 67.31 |
| 🏅️ |      ChatGLM-130B      | 71.39 | 48.67 | 68.07 | 75.00 | 83.44 | 84.68 | 95.94 | 67.16 | 45.60 | 70.86 | 72.12 |
| 🥈  |      文心一言(v2.2.0)      | 71.38 | 59.34 | 70.34 | 73.33 | 86.58 | 82.88 | 95.12 | 60.31 | 37.60 | 71.03 | 73.79 |
| 🥉  |       讯飞星火(v1.5)       | 65.72 | 47.32 | 68.38 | 77.42 | 72.03 | 69.09 | 89.43 | 59.85 | 35.77 | 71.23 | 63.46 |
|  4  |      ChatGLM2-6B       | 63.59 | 45.13 | 60.50 | 66.13 | 78.81 | 63.06 | 89.43 | 64.18 | 33.60 | 64.24 | 66.35 |
|  -  |     gpt-3.5-turbo      | 63.19 | 46.02 | 69.75 | 75.81 | 75.50 | 57.66 | 89.43 | 55.97 | 36.00 | 57.62 | 66.35 |
|  5  |    MiniMax-abab5.5     | 62.79 | 46.90 | 57.98 | 63.71 | 75.50 | 71.17 | 86.99 | 60.45 | 41.60 | 58.94 | 62.50 |
|  6  |       360智脑(4.0)       | 62.54 | 45.45 | 63.83 | 63.53 | 71.43 | 70.73 | 97.06 | 60.47 | 38.46 | 64.96 | 73.21 |
|  7  |          通义千问          | 61.73 | 41.59 | 60.87 | 60.66 | 73.65 | 67.89 | 88.24 | 51.91 | 40.68 | 68.97 | 57.89 |
|  8  |    internlm-chat-7b    | 61.35 | 41.59 | 58.82 | 62.10| 76.16 | 68.47 | 86.18 | 61.94 | 32.80 | 57.62 | 65.38 |
|  -  |        Claude-2        | 61.18 | 48.67 | 70.94 | 70.16 | 67.55 | 54.05 | 83.74 | 58.21 | 36.00 | 60.67 | 59.62 |
|  -  |   Claude-instant-v1    | 55.91 | 43.36 | 62.16 | 72.13 | 62.91 | 50.91 | 84.87 | 47.73 | 31.20 | 56.38 | 45.19 |
|  9  |   Baichuan-13B-Chat    | 55.38 | 45.13 | 58.82 | 50.81 | 73.51 | 70.27 | 75.61 | 47.01 | 33.60 | 44.37 | 54.81 |
| 10  |       BELLE-13B        | 52.99 | 42.48 | 55.46 | 67.74 | 56.29 | 46.85 | 78.05 | 38.06 | 33.60 | 59.60 | 49.04 |
| 11  |   IDEA-姜子牙-13B-v1.1    | 48.61 | 28.32 | 54.62 | 51.61 | 56.29 | 51.35 | 63.41 | 42.54 | 36.00 | 48.34 | 51.92 |
| 12  |       Phoenix-7B       | 44.62 | 30.09 | 51.26 | 43.55 | 51.66 | 45.95 | 65.85 | 35.07 | 32.00 | 45.03 | 44.23 |
| 13  |        MOSS-16         | 38.01 | 32.74 | 43.70 | 36.29 | 40.40 | 32.43 | 60.98 | 32.09 | 31.20 | 31.13 | 40.38 |
| 14  |    Llama-2-13B-chat    | 37.37 | 31.86 | 40.34 | 49.19 | 37.75 | 33.33 | 43.90 | 32.09 | 32.00 | 33.77 | 40.38 |
| 15  |       Vicuna-13B       | 33.71 | 21.24 | 34.45 | 45.16 | 29.14 | 22.52 | 33.33 | 36.57 | 22.40 | 49.67 | 38.46 |
| 16  | RWKV-7B-World-CHNtuned | 28.13 | 25.66 | 26.05 | 25.00 | 29.80 | 26.13 | 45.53 | 17.16 | 20.00 | 36.42 | 27.88 |

     注：外代表性非开源模型（GPT4.0/Claude/gpt-3.5/）参与榜单，但不参与排名；；指标为：Accuracy。数据表比较大，表格请往后啦。       


### SuperCLUE的构成与特点
着眼于综合评价大模型的能力，使其能全面地测试大模型的效果，又能考察模型在中文特有任务上的理解和积累。我们对能力进行了划分，
SuperCLUE从三个不同的维度评价模型的能力：基础能力、专业能力和中文特性能力。

#### 基础能力:

包括了常见的有代表性的模型能力，如语义理解、对话、逻辑推理、角色模拟、代码、生成与创作等10项能力。

#### 专业能力:

包括了中学、大学与专业考试，涵盖了从数学、物理、地理到社会科学等50多项能力。

#### 中文特性能力:

针对有中文特点的任务，包括了中文成语、诗歌、文学、字形等10项多种能力。

#### SuperCLUE的特点：
1）多个维度能力考察（3大类，70+子能力）：从三个不同角度对中文大模型进行测试，以考察模型的综合能力；并且每一个子能力又含有十项或以上不同的细分能力。

2）自动化测评（一键测评）：通过自动化测评方式以相对客观形式测试不同模型的效果，可以一键对大模型进行测评。

3）广泛的代表性模型（17个模型）：选取了多个国内外有代表性的可用的模型进行测评，以反映国内大模型的发展现状并了解与国际领先模型的差距或相对优劣势。

4）人类基准：在通用人工智能发展的背景下，SuperCLUE也提供了模型相对于人类效果的指标对比。

### 数据集数量
    2023年6月榜单，针对基础能力、中文特性和专业能力，使用了3714题目。
    
## SuperCLUE的数据集
1.基础能力（10项能力）：语义理解、生成与创作、闲聊、对话、百科与知识、逻辑与推理、计算能力、代码、角色模拟、安全
    
    示例：
    语义理解：
        两个男人正常交谈，其中一个男人夸赞对方办事能力强，对方回答“哪里，哪里”。这里的“哪里，哪里”是什么意思？
        A. 讲话十分含糊不清。
        B. 要求说出具体的优点。
        C. 表达自己的谦虚。
        D. 挑衅对方。
         
    逻辑与推理：
        小明的妻子生了一对双胞胎。以下哪个推论是正确的？
        A. 小明家里一共有三个孩子。
        B. 小明家里一共有两个孩子。
        C. 小明家里既有男孩子也有女孩子。
        D. 无法确定小明家里孩子的具体情况。
 
     

2.中文特性能力（10项能力）：成语、诗词、文学、字义理解、汉语句法分析、汉字字形和拼音理解、歇后语和谚语、对联、方言、古文
    
    示例：
    成语：
    选出下列句子中成语使用错误的一项
        A. 这个项目时间紧任务重，大家都在马不停蹄地奔波劳碌。
        B. 他常常口是心非，让人难以相信他说的话。
        C. 两人是同学三年，一直保持着良好的关系，相互尊重、相敬如宾。
        D. 当地突发大火，整个村庄都鸡犬不宁，局势十分危急。
     
    文学：
    下列有关名著的表述有误的一项是
        A. 《红楼梦》是中国古代小说中的巅峰之作，以其瑰丽的语言和丰富的人物形象而闻名于世。
        B. 《西游记》是中国古代四大名著之一，讲述了哪吒等人历经九九八十一难，最终取得真经的故事。
        C. 《孔乙己》是鲁迅的代表作之一，以其深刻的社会洞察力和优美的文学风格而广受好评。
        D. 《围城》是钱钟书的代表作之一，以其独特的文学语言和深刻的社会洞察力而成为现代中国文学的经典之作。
         
     
3.专业能力（50+能力）：抽象代数、天文学、临床知识、大学生物学、大学计算机科学、大学数学、高中化学、高中物理、机器学习、营养、专业会计、职业心理学等
    
    示例：
    物理：
    以下物理常识题目，哪一个是错误的?
        A. 在自然环境下，声音在固体中传播速度最快。
        B. 牛顿第一定律：一个物体如果不受力作用，将保持静止或匀速直线运动的状态。
        C. 牛顿第三定律：对于每个作用力，都有一个相等而反向的反作用力。
        D. 声音在空气中的传播速度为1000m/s。
     
    天文学：
    以下天文学常识题目，哪一个是错误的？
        A. 太阳系是指由太阳和围绕着它运行的八大行星、矮行星、卫星、小行星带和彗星组成的一个行星系统。
        B. 卫星是指绕行星或其他天体运动的天体。
        C. 彗星是指太阳系中一种较小的天体，其核心由冰和尘埃组成。
        D. 按一般的天体归类方法，月球属于行星。
    
## SuperCLUE全自动测评过程：
    1、统一prompt：针对每一个题目，构造了统一的prompt供模型和人类使用；
    2、预测：系统使用模型进行预测，要求模型选取ABCD中的某一个选项；
    3、打分：如果模型的回答不是标准的答案，而是一段文字，系统会采取特定的策略自动提取出模型的答案。该策略结合模型的表现进行优化和完善。
      （注：当无法提取有效答案的时候，则表明模型没有按照人类的要求做题，未正确理解指令，则认为模型回答错误。）
       
   由于此次为SuperCLUE首次全自动测评，为了谨慎起见，全部答案事后已由多位人类进行交叉复核，与自动测评结果基本一致。

## 人类基准测评
针对于基础能力和中文特性能力题目，会有三位独立的人类测评员根据题目作答。人类测评结果，采用多数投票方式进行汇总，作为人类基准分数。

## 实验分析

#### 人类与模型的对比

从人类测评角度看，基础能力（92%）+ 中文特性能力（94%），都达到了非常高的水平。除GPT-4外，人类准确率大幅超过了其他的大模型（如在基础能力上超过其他模型15多个百分点）。
 AI虽然进展很快，但人类还是有相对优势的， 比如在计算方面，人类比最强模型GPT-4高出了30个百分点。

   
#### 模型层面-宏观分析

一句话点评：国际先进模型的效果具有较大的领先性；同时国产GPT模型也有不俗的表现，有差距但可追赶。

1）中文大模型的必要性

在国际上效果非常棒的Vicuna-13B模型，在中文领域的效果是众多模型中比较一般的模型（排名靠后）。而国内研发的大模型或在中文任务上进行训练后的模型，都大幅超过了Vicuna-13B的效果，比如星火认知大模型在总分上超过了 Vicuna-13B 20个百分点，并且BELLE-13B（基于LLaMA并在中文上训练和微调过的模型）的总分也超过了 Vicuna-13B 10多个百分点。
    
2）国内大模型与OpenAI GPT之间的差距较大，但在逐渐逼近

 可以看到在本次SuperCLUE上效果最好的国内模型，星火认知大模型，与GPT-4相比有23个百分点的差距，与gpt-3.5-turbo在总分上也有13个百分点的差距。但是我们更应该看到，
 不断涌现和迭代的国内大模型也在逐步地缩小与OpenAI GPT模型模型的差距。
 
3）gpt-3.5-turbo与GPT-4之间也有明显差距

   比如，GPT-4在所有参与测评的模型中是独一档的存在，超过了gpt-3.5-turbo近10个百分点。它在逻辑推理能力、生成与创作能力方面，远远优于其他模型（超过其他模型20个百分点或以上）。
   

#### 能力角度分析

1） 当前模型在基础能力普遍表现不错，但中文特性能力、专业能力还比较差。

   说明当前国内大模型已经有不错的基础（60-70%），但在专业领域、中文任务上表现一般（如30-60%直接），说明在专业领域或中文任务上还需要继续努力，或者说进行针对性的训练。
   
2）当前模型通常在逻辑推理、计算方面能力较差。

  除GPT-4外，其他模型在这两项能力上通常在30-50分之间。

3）角色模拟，AI模型比较擅长。
  这方面可以是非常有用的。可以让AI根据场景和角色设定帮忙人类来完成多种不同的任务，例如市场营销策划、心理咨询、客户服务、到提供创意或想法等。

#### 国内大模型点评

本次测评中，国内大模型中360智脑、讯飞星火认、文心一言、MiniMax模型有不错的表现。


## SuperCLUE的不足与局限
1. 基础能力、中文特性能力：虽然每一部分都包含了10类子能力，但这两个能力的总数据量比较少，可能存在需要扩充数据集的问题。
2. 选取模型的不完全：我们测试了9个模型，但还存在着更多的可用中文大模型。需要后续进一步添加并测试；有的模型由于没有广泛对外提供服务，我们没能获取到可用的测试版本。
3. 选取的能力范围：我们尽可能的全面、综合衡量模型的多维度能力，但是可能有一些模型能力没有在我们的考察范围内。后续也存在扩大考察范围的可能。
4. 客观考察的不足：我们以相对客观形式考察模型能力，但一些主观、开放性问题的模型能力的考察可能存在不足。
5. 模型参数：当前大模型发展较快，参数量又有比较大的差异，本次的测评并没有在同一级别的参数量上进行。

## SuperCLUE的常见问答

1. 什么时候会公布评测集和更多细节？

       由于本轮评测尚未结束，数据集和进一步信息计划将在本轮SuperCLUE评测结束后公开，敬请期待。     

2. 测试方法
       
       相同的prompt情况下，让不同的模型对题目进行预测结果，与正确答案进行匹配，计算最终结果，并统计准确率（ACC）。
       计算正确答案：根据模型预测结果，系统会提取答案，并计算题目的分数；为稳妥起见，人工会符合每一个模型的预测结果及其答案。

3. 为什么人工测评的成绩这么高? 人类测评员是什么水平？
       
       当前报告的人类测评的分数是采取开卷考试形式的进行的。即由每一个题目3个人类测评员进行开卷考试，最后结果进行多数投票后获得。
       我们也会添加采取闭卷形式的人类分数。
       
       人类测评员是高年级本科生、研究生水平。

       

## SuperCLUE讨论与交流

SuperCLUE榜单大模型评测申请：https://wj.qq.com/s2/12305633/a73d/

模型内测需求收集：https://wj.qq.com/s2/12307825/2ae0/



<p float="left">   
  <img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/superclue6group.jpeg"  width="30%" height="30%"></img>
  <img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/brightmart_s.jpeg"  width="30%" height="30%"></img>
</p> 

 
<a href="https://discord.gg/GPHv9BfNUD" target="__blank">Discord SuperCLUE交流群</a>

## 引用

如果使用本项目的，请引用本项目。

    @misc{SuperCLUE,
      author = {Liang Xu, Xuanwei Zhang, Kangkang Zhao, Lei Zhu and others from SuperCLUE team},
      title = {SuperCLUE: A Benchmark for Foundation Models in Chinese},
      year = {2023},
      publisher = {GitHub},
      journal = {GitHub repository},
      howpublished = {\url{https://github.com/CLUEbench/SuperCLUE}},
    }
