# SuperCLUE

中文通用大模型综合性基准SuperCLUE

<a href='https://www.superclueai.com' target="__blank">榜单地址</a>

公众号文章：<a href='https://mp.weixin.qq.com/s/VEqF1RriFpP2pOO_cvHo8Q'>SuperCLUE中文大模型测评基准10月榜单发布</a>

官网地址：<a href='https://www.cluebenchmarks.com/superclue.html' target="__blank">www.cluebenchmarks.com/superclue.html</a>

技术报告：<a href='https://arxiv.org/abs/2307.15020' target="__blank">SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark</a>

【2023-10-25】 发布SuperCLUE-2023年10月榜单


【2023-10-19】 <a href='https://www.cluebenchmarks.com/superclue_agent.html' target="__blank">SuperCLUE-Agent：Agent智能体中文原生任务评估基准</a>


【2023-9-12】 <a href='https://github.com/CLUEbenchmark/SuperCLUE-safety' target="__blank">SuperCLUE-Safety：中文大模型多轮对抗安全基准</a>


【2023-9-26】，SuperCLUE发布中文大模型9月榜单。

SuperCLUE是一个综合性大模型评测基准，本次评测主要聚焦于大模型的四个能力象限，包括语言理解与生成、专业技能与知识、Agent智能体和安全性，进而细化为12项基础能力。

相比与上月，新增了AI Agent智能体

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/superclue_idea.jpeg"  width="90%" height="90%"></img>

### SuperCLUE能力评估结构图
<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/category09.png"  width="60%" height="60%"></img>

### SuperCLUE多维度测评方案
<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/r2309/superclue_mlitisystem.png"  width="90%" height="90%"></img>


### 为什么新增AI Agent智能体能力？

AI agent（智能体）是当前与大语言模型相关的前沿研究热点，拥有类似贾维斯等科幻电影中人类超级助手的能力，可以根据需求自主的完成任务。
然而，面向AI agent智能体，缺乏针对中文大模型的广泛评估。为了解决这一问题，我们在SuperCLUE新的榜单中新增了AI agent智能体能力的测评。
这个榜单将重点评估AI agent在【工具使用】和【任务规划】两个关键能力上的表现，这项工作旨在为评估中文大模型作为智能体的表现提供一个基础和可能。

### SuperCLUE总排行榜（2023年10月）
| 排名 | 模型 | 机构 | 总分 | OPEN<br/>多轮开放 | OPT<br/>客观题 | 使用 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| - | GPT4 | OpenAI | 87.08 | 88.07 | 85.60 | API |
| - | Claude2 | Anthropic | 72.46 | 75.11 | 68.48 | API |
| - | GPT3.5 | OpenAI | 71.12 | 73.12 | 68.13 | API |
| 🏅️ | BlueLM | vivo | 70.74 | **66.78** | 76.67 | 申请 |
| 🥈 | Moonshot | 月之暗面 | 70.42 | 66.02 | 77.03 | 网页 |
| 🥉 | 文心一言4.0 | 百度 | 69.26 | 61.81 | **80.44** | API |
| 🥉 | SenseChat 3.0 | 商汤科技 | 69.25 | 63.16 | 78.39 | API |
| 5 | ChatGLM2-Pro | 清华&智谱 | 65.93 | 58.53 | 77.02 | API |
| 6 | 云雀大模型（豆包） | 字节跳动 | 64.39 | 59.11 | 72.30 | 网页 |
| 7 | 讯飞星火V3.0 | 科大讯飞 | 63.99 | 59.26 | 71.08 | API |
| 8 | Baichuan2-13B-Chat | 百川智能 | 62.70 | 57.77 | 70.09 | 模型 |
| 9 | MiniMax-Abab5.5 | MiniMax | 59.57 | 48.13 | 76.72 | API |
| 10 | 通义千问plus | 阿里巴巴 | 57.09 | 43.36 | 77.68 | API |
| 11 | Qwen-14B-Chat | 阿里巴巴 | 56.97 | 43.10 | 77.78 | API |
| 12 | 讯飞星火V2.0 | 科大讯飞 | 55.24 | 47.95 | 66.18 | API |
| 13 | OpenBuddy-70B | OpenBuddy | 53.34 | 45.14 | 65.65 | 模型 |
| 14 | Chinese_Alpaca2_13B | yiming cui | 47.27 | 41.95 | 55.26 | 模型 |
| 15 | 360GPT_S2_V9 | 360 | 43.79 | 28.44 | 66.82 | API |
| 16 | ChatGLM2-6B | 清华&智谱 | 42.27 | 29.48 | 61.45 | 模型 |
| - | Llama2-13B-Chat | Meta | 36.46 | 33.91 | 40.28 | 模型 |

注：处于前列的模型，如果分数比较接近（小于0.03分），在排名时会被记为并列的名称。

### SuperCLUE-OPEN多轮开放问题排行榜（2023年10月）
| 排名 | 模型 | 机构 | OPEN<br/>总分 | 语言<br/>理解 | 专业技能<br/>与知识 | 工具<br/>使用 | 传统<br/>安全 | 使用 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| - | GPT4 | OpenAI | 88.07 | 81.01 | 94.90 | 88.75 | 88.27 | API |
| - | Claude2 | Anthropic | 75.11 | 65.02 | 84.56 | 62.50 | 90.31 | API |
| - | GPT3.5 | OpenAI | 73.12 | 67.27 | 79.49 | 56.88 | 87.24 | API |
| 🏅️ | BlueLM | vivo | 66.78| **55.22** | 75.07 | 59.12 | 87.48 | 申请 |
| 🥈 | Moonshot | 月之暗面 | 66.02 | 53.07 | 72.92 | **71.25** | 84.95 | 网页 |
| 🥉 | SenseChat 3.0 | 商汤科技 | 63.16 | 48.58 | 69.79 | 71.15 | 86.99 | API |
| 4 | 文心一言4.0 | 百度 | 61.81 | 37.07 | **77.95** | 69.62 | 88.40 | API |
| 5 | 讯飞星火V3.0 | 科大讯飞 | 59.26 | 41.32 | 73.48 | 48.75 | 84.69 | API |
| 6 | 云雀大模型（豆包） | 字节跳动 | 59.11 | 50.71 | 63.07 | 43.12 | **92.86** | API |
| 7 | ChatGLM2-Pro | 清华&智谱 | 58.53 | 44.59 | 67.26 | 51.95 | 85.97 | API |
| 8 | Baichuan2-13B-Chat | 百川智能 | 57.77 | 52.76 | 56.81 | 62.50 | 76.92 | 模型 |
| 9 | MiniMax-Abab5.5 | MiniMax | 48.13 | 32.67 | 56.89 | 50.63 | 72.45 | API |
| 10 | 讯飞星火V2.0 | 科大讯飞 | 47.95 | 33.23 | 54.53 | 43.75 | 84.69 | API |
| 11 | OpenBuddy-70B | OpenBuddy | 45.14 | 25.45 | 54.51 | 56.33 | 75.26 | 模型 |
| 12 | 通义千问plus | 阿里巴巴 | 43.36 | 21.98 | 55.34 | 45.62 | 78.72 | API |
| 13 | Qwen-14B-Chat | 阿里巴巴 | 43.10 | 21.98 | 53.94 | 50.00 | 77.30 | API |
| 14 | Chinese_Alpaca2_13B | yiming cui | 41.95 | 40.77 | 39.90 | 21.25 | 75.51 | 模型 |
| - | Llama2-13B-Chat | Meta | 33.91 | 33.17 | 26.17 | 30.52 | 71.17 | 模型 |
| 15 | ChatGLM2-6B | 清华&智谱 | 29.48 | 19.29 | 31.66 | 10.62 | 80.36 | 模型 |
| 16 | 360GPT_S2_V9 | 360 | 28.44 |	14.47 |	32.40 |	17.31 |	79.59 |	API |

### SuperCLUE-OPT三大能力客观题排行榜（2023年10月）
| 排名 | 模型 | 机构 | OPT分数 | 基础能力 | 中文特性 | 学术与<br/>专业能力 | 使用 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| - | GPT4 | OpenAI | 85.60 | 92.83 | 86.48 | 77.82 | API |
| 🏅️ | 文心一言4.0 | 百度 | 80.44 | **90.02** | **89.62** | 62.78 | API |
| 🥈 | SenseChat 3.0 | 商汤科技 | 78.39 | 87.79 | 85.41 | **62.95** | API |
| 🥉 | Qwen-14B-Chat | 阿里巴巴 | 77.78 | 88.96 | 85.71 | 59.38 | API |
| 4 | 通义千问plus | 阿里巴巴 | 77.68 | 87.56 | 85.81 | 60.47 | API |
| 5 | Moonshot | 月之暗面 | 77.03 | 85.60 | 83.74 | 62.35 | 网页 |
| 6 | ChatGLM2-Pro | 清华&智谱 | 77.02 | 86.65 | 86.48 | 59.03 | API |
| 7 | MiniMax-Abab5.5 | MiniMax | 76.72 | 87.23 | 83.45 | 60.48 | API |
| 8 | BlueLM | vivo | 76.67 | 86.61 | 85.11 | 59.36 | 申请 |
| 9 | 云雀大模型 | 字节跳动 | 72.30 | 81.09 | 78.93 | 57.64 | API |
| 10 | 讯飞星火V3.0 | 科大讯飞 | 71.08 | 82.23 | 81.19 | 49.80 | API |
| 11 | Baichuan2-13B-Chat | 百川智能 | 70.09 | 80.20 | 80.41 | 50.92 | 模型 |
| - | Claude2 | Anthropic | 68.48 | 81.04 | 67.62 | 57.35 | API |
| - | GPT3.5 | OpenAI | 68.13 | 82.80 | 67.91 | 54.53 | API |
| 12 | 360GPT_S2_V9 | 360 | 66.82 | 82.50 | 73.43 | 44.65 | API |
| 13 | 讯飞星火V2.0 | 科大讯飞 | 66.18 | 80.04 | 73.74 | 45.24 | API |
| 14 | OpenBuddy-70B | OpenBuddy | 65.65 | 83.83 | 63.57 | 50.32 | 模型 |
| 15 | ChatGLM2-6B | 清华&智谱 | 61.45 | 75.53 | 67.09 | 42.83 | 模型 |
| 16 | Chinese_Alpaca2_13B | yiming cui | 55.26 | 69.74 | 56.96 | 39.93 | 模型 |
| - | Llama2-13B-Chat | Meta | 40.28 | 51.74 | 36.14 | 33.30   | 模型 |


### SuperCLUE十大基础能力排行榜（2023年10月）
| 模型 | 机构 | 计算 | 逻辑<br/>推理 | 代码 | 知识<br/>百科 | 语言<br/>理解 | 生成<br/>创作 |对话 | 角色<br/>扮演 | 工具<br/>使用 | 传统<br/>安全 |
|:---: |:---: |:---: |:---: |:---: |:---: |:---: |:---: |:---: |:---: |:---: |:---: |  
| GPT4 | OpenAI | 95.56 | 100.00 | 85.89 | 98.14 | 100.00 | 68.68 | 75.68 | 79.68 | 88.75 | 88.27 |
| Claude2 | Anthropic | 75.48 | 100.00 | 74.63 | 88.14 | 84.91 | 46.58 | 67.42 | 61.16 | 62.50 | 90.31 |
| GPT3.5 | OpenAI | 74.04 | 95.10 | 69.25 | 79.56 | 87.61 | 55.65 | 59.26 | 66.57 | 56.88 | 87.24 |
| BlueLM | vivo | 58.52 | 90.11 | **60.91** | 90.73 | 68.52 | **40.32** | **59.84** | 52.21 | 59.12 | 87.48 |
| 文心一言4.0 | 百度 | **71.30** | 98.61 | 60.81 | 81.08 | 70.65 | 18.42 | 30.26 | 28.95 | 69.62 | **88.40** |
| SenseChat 3.0 | 商汤科技 | 43.40 | 88.16 | 58.57 | 89.02 | 81.82 | 27.63 | 37.50 | 47.37 | 71.15 | 86.99 |  
| MiniMax-Abab5.5 | MiniMax | 34.26 | 63.51 | 47.37 | 82.43 | 54.35 | 21.05 | 26.32 | 28.95 | 50.63 | 72.45 |
| OpenBuddy-70B | OpenBuddy | 31.48 | 89.19 | 47.37 | 50.00 | 47.83 | 9.21 | 28.95 | 15.79 | 56.33 | 75.26 |
| Moonshot | 月之暗面 | 64.81 | **100.00** | 44.74 | 82.14 | **88.04** | 31.08 | 52.63 | 40.54 | **71.25** | 84.95 |
| Qwen-14B-Chat | 阿里巴巴 | 52.78 | 52.86 | 44.74 | 65.38 | 46.74 | 14.47 | 14.86 | 11.84 | 50.00 | 77.30 |
| 讯飞星火V3.0 | 科大讯飞 | 68.52 | 85.53 | 43.42 | **96.43** | 58.70 | 27.63 | 28.95 | 50.00 | 48.75 | 84.69 |  
| ChatGLM2-Pro | 清华&智谱 | 64.81 | 90.54 | 36.84 | 76.83 | 65.22 | 25.00 | 48.68 | 39.47 | 51.95 | 85.97 |
| Baichuan2-13B-Chat | 百川智能 | 50.93 | 80.26 | 36.84 | 59.21 | 66.30 | 32.89 | 57.89 | **53.95** | 62.50 | 76.92 |
| 通义千问plus | 阿里巴巴 | 46.30 | 70.00 | 35.53 | 69.51 | 51.09 | 3.95 | 21.05 | 11.84 | 45.62 | 78.72 |
| Chinese_Alpaca2_13B | yiming cui | 24.07 | 52.70 | 35.53 | 47.30 | 67.39 | 18.42 | 40.79 | 36.49 | 21.25 | 75.51 |  
| Llama2-13B-Chat | Meta | 7.41 | 48.53 | 32.89 | 15.85 | 60.87 | 26.32 | 28.38 | 17.11 | 30.52 | 71.17 |
| 讯飞星火V2.0 | 科大讯飞 | 51.85 | 55.41 | 31.58 | 79.27 | 50.00 | 28.95 | 28.95 | 25.00 | 43.75 | 84.69 |
| 云雀（豆包） | 字节跳动 | 43.52 | 93.42 | 26.32 | 89.02 | 88.04 | 12.16 | 50.00 | 52.63 | 43.12 | 92.86 |
| ChatGLM2-6B | 清华&智谱 | 18.52 | 58.11 | 25.00 | 25.00 | 52.17 | 6.58 | 7.89 | 10.53 | 10.62 | 80.36 |
| 360GPT_S2_V9 | 360 | 13.89 | 64.86 | 16.22 | 34.62 | 25 | 2.63 | 21.05 | 9.21 | 17.31 | 79.59 |


### SuperCLUE开源模型排行榜（2023年10月）

| 排名 | 模型 | 机构 | 总分 | OPEN<br>多轮开放 | OPT<br>客观题 | 使用 |
|:---: |:---: |:---: |:---: |:---: |:---: |:---: |
| - | GPT4 | OpenAI | 87.08 | 88.07 | 85.60 | API |
| - | Claude2 | Anthropic | 72.46 | 75.11 | 68.48 | API |
| - | GPT3.5 | OpenAI | 71.12 | 73.12 | 68.13 | API |
| 🏅 | Baichuan2-13B-Chat | 百川智能 | 62.70 | 57.77 | 70.09 | 模型 |
| 🥈 | Qwen-14B-Chat | 阿里巴巴 | 56.97 | 43.10 | 77.78 | API |  
| 🥉 | OpenBuddy-70B | OpenBuddy | 53.34 | 45.14 | 65.65 | 模型 |
| 4 | Chinese_Alpaca2_13B | yiming cui | 47.27 | 41.95 | 55.26 | 模型 |
| 5 | ChatGLM2-6B | 清华&智谱| 42.27 | 29.48 | 61.45 | 模型 |
| - | Llama2-13B-Chat | Meta | 36.46 | 33.91 | 40.28 | 模型 |
 

### 23-10月测评改进

#### 1. 模型变动
1）本次评测选取了目前国内外最具代表性的20个通用大语言模型。与9月相比，新增了月之暗面的Moonshot、百度的文心一言4.0、科大讯飞的星火V3.0、
vivo的vivoLM和阿里云的Qwen-14B。
 
具体被测模型的配置信息见Github的ModelCard。Github地址：https://github.com/CLUEbenchmark/SuperCLUE

#### 2. 评测任务变动
本月评测任务新增AI智能体，重点评估AI Agent在【工具使用】这个关键能力上的表现；大模型安全使用【传统安全】这一关键能力。

#### 3. 评分机制变动（与9月一致）
SuperCLUE结合大模型市场技术进展及国内外评测基准现状，对综合性评测总分评分逻辑进行优化。

1）多轮开放评测OPEN评分标准：在与基线模型对战过程中，我们认为胜的情况价值意义更大。所以，本次OPEN测评将胜（1分）调整为胜（3分）。如一道题目对战，胜得3分，平局得1分，负得0分。

2）我们发现客观选择题并不能考察中文大模型的真实综合能力，多轮主观题的能力尤为重要，所以我们在计算总分时，将OPEN的权重由50%提升至60%。


### 示例
#### 能力1：语义理解与抽取

这是一种语言能力，能够理解并解析输入的文字信息的含义。模型需要能够识别短语、句子、段落的含义，同时还要能从更大的文本块中抽取关键信息和主题。

##### 多轮对话示例

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/r2309/image_nlp.png"  width="100%" height="100%"></img>

注：本示例中可同时评测多轮对话能力

#### 能力2：AI agent（智能体）能力

AI agent（智能体）是当前与大语言模型相关的前沿研究热点，拥有类似贾维斯等科幻电影中人类超级助手的能力，可以根据需求自主的完成任务。

重点评估AI agent在【工具使用】和【任务规划】两个关键能力上的表现

##### 示例

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/r2309/image_agent.png"  width="100%" height="100%"></img>


#### 能力3：上下文对话

这是一种语言能力，需要理解并记住前面的对话信息，以便在回答中保持连贯性。这涉及到理解对话的整体流程和上下文环境，或生成相应的对话。

##### 示例

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/r2309/image_dial.png"  width="100%" height="100%"></img>

#### 能力4：生成与创作

这是一种语言能力，能够创造新的文本内容，如文章、文案、短故事、诗歌。这涉及到创造性地运用语言，同时还要考虑到风格、语境和目标读者。

##### 示例
<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/r2309/image_generate.png"  width="100%" height="100%"></img>


#### 能力5：知识与百科

这是一种知识能力，能够像百科全书一样提供知识信息。这涉及到理解和回答关于广泛主题的问题，以及提供准确、详细和最新的信息。

##### 示例

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/r2309/image_knowledge.png"  width="100%" height="100%"></img>


#### 能力6：代码

这是一种专业能力，能够理解和生成编程代码。这涉及到理解多种编程语言的语法、结构和习惯，以及如何解决编程问题。

##### 多轮对话示例

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/r2309/image_code.png"  width="100%" height="100%"></img>

注：本示例中可同时评测多轮对话能力

#### 能力7：逻辑与推理

这是一种专业能力，能够理解和应用逻辑原则进行推理。这涉及到分析问题、识别问题及推理。

##### 示例

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/r2309/image_logic.png"  width="100%" height="100%"></img>


####  能力8：计算

这是一种专业能力，使其能够执行数学运算，如加法、减法、乘法和除法，甚至更复杂的数学问题。这涉及到理解数学问题的表述，以及如何步骤地解决这些问题。

##### 多轮对话示例

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/r2309/image_compute.png"  width="100%" height="100%"></img>

注：本示例中可同时评测多轮对话能力

####  能力9：角色扮演

这是一种感知能力，使其能够在特定的模拟环境或情景中扮演一个角色。这涉及到理解特定角色的行为、说话风格，以及在特定情境下的适当反应。

##### 示例

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/r2309/image_roleplay.png"  width="100%" height="100%"></img>


####   能力10：安全

这是一种安全能力，防止生成可能引起困扰或伤害的内容。这涉及到识别和避免可能包含敏感或不适当内容的请求，以及遵守用户的隐私和安全政策。

##### 示例

<img src="https://github.com/CLUEbenchmark/SuperCLUE/blob/main/resources/r2309/image_safety.png"  width="100%" height="100%"></img>

### 8月榜单更新情况
1.综合性：将OPEN多轮开放问题与OPT三大能力客观题进行了结合起来，作为8月榜单；

2.模型细节：Baichuan-13B-Chat使用了是最新的模型权重，具体见huggingface的权重；文心一言，OPT三大能力客观题使用的是API（Ernie-3.5-turbo）；
  360使用的是api版本；

3.模型更新：去除了一些前期大家比较关注但当前活跃度不高的模型，如MOSS，BELLE等；加入了一些如Qwen-7B-Chat和3个Llam2相关模型。
